{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from alexnet import alexnet\n",
    "from DataGenerator import DataGenerator\n",
    "from datetime import datetime\n",
    "from tensorflow.contrib.data import Iterator\n",
    "from tensorflow.contrib.data.Dataset import Dataset\n",
    "\n",
    "# data file path in computer \n",
    "train_file = \" \"\n",
    "valid_file = \" \"\n",
    "\n",
    "# parameters for model\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "dropout_rate = 0.5\n",
    "num_classes = 1000\n",
    "train_layers = [\"fc6\", \"fc7\", \"fc8\"]  # not use the fine-tuned wieghts \n",
    "\n",
    "## setting the checkpoint\n",
    "display_step = 10\n",
    "# tensorboard and checkpoint path in computer\n",
    "filewriter_path = \" \"\n",
    "checkpoint_path = \" \"\n",
    "\n",
    "# check if the tensorboard and checkpoint exist and if not, create the folder\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "if not os.path.isdir(filewriter_path):\n",
    "    os.mkdir(filewriter_path)\n",
    "\n",
    "# methods for data \n",
    "train = DataGenerator(train_file, batch_size = batch_size, \n",
    "                           num_classes = num_classes)\n",
    "valid = DataGenerator(valid_file, batch_size = batch_size, \n",
    "                          num_classes = num_classes, shuffle = false)\n",
    "\n",
    "# get the data from train and valid instance\n",
    "train_data = train.data\n",
    "valid_data = valid.data\n",
    "\n",
    "## Iterator with reinitializabel method \n",
    "# get the batch data for model\n",
    "Iter = Iterator.from_structure(train_data.output_type, train_data.output_shape)\n",
    "next_batch = Iter.get_next()\n",
    "# Ops for initializing the two iterators\n",
    "train_init_op = iterator.make_initializer(train_data)\n",
    "valid_init_op = iterator.make_initializer(valid_data)\n",
    "\n",
    "# placeholders for alexnet model\n",
    "x = tf.placeholder(tf.float32, shape = [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [batch_size, num_classes])\n",
    "keep_pro = tf.placeholder(tf.float32)\n",
    "\n",
    "# initialize the alexnet model\n",
    "model = alexnet(x, num_classes, keep_pro, train_layers)\n",
    "\n",
    "# output of the last fully cinnected layer\n",
    "out = model.out\n",
    "\n",
    "# lists for trianable layer (fully connected layers)\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split(\"/\")[0] in train_layers]\n",
    "\n",
    "# Op for loss \n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = out, labels = y))\n",
    "    \n",
    "# build the optimizer for model\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.trian.GradientDescentOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss, var_list)    \n",
    "    gradients = [(MyCapper(gv[0]), gv[1]) for gv in gradients]\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars = gradients)\n",
    "    \n",
    "# Evaluation method: evaluating the model\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# summary setting\n",
    "# gradients\n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + \"/gradient:\", gradient)\n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "# loss\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "# accuracy\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "# merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# initialize a filewriter and saver\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "saver = tf.summary.Saver()\n",
    "\n",
    "# get training and validation steps in one epoch\n",
    "train_batch = int(np.floor(train.num_samples/batch_size))\n",
    "valid_bacth = int(np.floor(valid.num_samples/batch_size))\n",
    "\n",
    "# Start the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # write the graph to the tensorboard\n",
    "    writer.add_graph(sess.graph)\n",
    "    # load the fine-tune weights to model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    print(\"{} start training the model ... \".format(datetime.now()))\n",
    "    print(\" open tensorboard at -- logdir{} .\".format(filewriter_path))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"{} Epoch number: {} ... \".format(datetime.now(), epoch))\n",
    "        \n",
    "        # Initialize the iterator for dataset\n",
    "        sess.run(train_init_op)\n",
    "        \n",
    "        for step in range(train_batch):\n",
    "            img, label = sess.run(next_batch)\n",
    "            sess.run(train_op, feed_dict = {x: img, y: label, keep_pro = dropout_rate})\n",
    "            \n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict = {x: img, y: label, keep_pro = 1.0})\n",
    "                writer.add_summary(s, epoch*train_batch+step)\n",
    "            \n",
    "        # validate the model on the validation dataset\n",
    "        print(\"{} start validating the model ...\")\n",
    "        sess.run(valid_inti_op)\n",
    "        valid_acc = 0\n",
    "        valid_count = 0\n",
    "        for _ in range(valid_batch):\n",
    "            img, label = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict = {x: img, y: label, keep_pro = 1.0})\n",
    "            # get accuracy for all data\n",
    "            valid_acc += acc\n",
    "            valid_count += 1\n",
    "            # get the average accuracy\n",
    "            valid_acc /= valid_count\n",
    "        print(\"validation accuracy = {:.4f} ... \".format(valid_acc))\n",
    "        print(\"{} saving checkpoints of model ... \".format(datetime.now()))\n",
    "        # saving checkpoints of model \n",
    "        checkpoint_name = os.path.join(checkpoint_pacth, \"epcoh \" + str(epoch+1) + \".ckpt\")\n",
    "        save_path = saver.save(sess, checkpoint_name)\n",
    "        print(\"Model checkpoint saved at {}\".format(checkpoint_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
