{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "# Convolutional layers\n",
    "def conv2d(x, kheight, kwidth, strideX, strideY, \n",
    "          featureDim, name, padding = \"SAME\", groups = 1): # groups for parallel working \n",
    "    channel = int(x.get_shape()[-1])\n",
    "    conv = lambda a, b: tf.nn.conv2d(a, b, strides = [1, strideY, strideY, 1], padding = padding)\n",
    "    with tf.Variable_scope(name) as scope:\n",
    "        w = tf.get_variable(\"w\", shape = [kheight, kwidth, channel/groups, featureDim])\n",
    "        b = tf.gat_variable(\"b\", shape = [featureDim])\n",
    "        # devide the output to several groups, in alexnet it's 2, then output channel should be half\n",
    "        xNew = tf.split(value = x, num_of_size_splits = groups, axis = 3)\n",
    "        wNew = tf.split(value = w, num_of_size_splits = groups, axis = 3)\n",
    "        # Convolutional operation \n",
    "        featureMap = [conv(t1, t2) for t1, t2 in zip(xNew, wNew)]\n",
    "        # concatenate the different groups into one output\n",
    "        mergeFeatureMap = tf.concat(values = featureMap, axis = 3)\n",
    "        out = tf.bias_add(mergeFeatureMap, b)\n",
    "        #return tf.nn.relu(out, scope.name)\n",
    "        return tf.nn.relu(tf.reshape(out, mergeFeatureMap.get_shape().as_list()), name = scope.name)\n",
    "\n",
    "# pooling layer\n",
    "def pool2d(x, kheight, kwidth, strideX, strideY, \n",
    "                name, padding = \"SAME\"):\n",
    "    return tf.nn.max_pool(x, ksize = [1, kheight, kwidth, 1], strides = [1, strideX, strideY, 1],\n",
    "                         padding = padding, name = name)\n",
    "\n",
    "# dropout function\n",
    "def dropout(x, keepPro, name = None):\n",
    "    return tf.nn.dropout(x, keep_prob = keepPro, name = name)\n",
    "\n",
    "# Normalization \n",
    "def LRN(x, R,  alpha, beta, name = None, bias = 1.0):\n",
    "    return tf.nn.local_response_normalization(x, depth_radius = R, alpha = alpha,\n",
    "                                             beta = beta, bias = bias, name = name)\n",
    "\n",
    "# fully connected layer\n",
    "def fcLayer(x, inputD, outputD, reluFlag, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        w = tf.get_variable(\"w\", shape = [inputD, outputD], dtype = tf.float32)\n",
    "        b = tf.get_variable(\"b\", shape = [outputD], dtype = tf.float32)\n",
    "        out = tf.bias_add(tf.matmul(x, w), b, name = scope.name)\n",
    "        if reluFlag:\n",
    "            return tf.nn.relu(out)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "# Model\n",
    "class alexNet(object):\n",
    "    def __init__(self, x, classNum, keepPro, skip, weights_path = \"DEFAULT\"):\n",
    "        self.x = x\n",
    "        self.classNum = classNum\n",
    "        self.keepPro = keepPro\n",
    "        self.skip = skip                         #skip layer won't use pretrained weights\n",
    "        \n",
    "        # Loading the pretrained weights\n",
    "        if weights_path == \"DEFAULT\":\n",
    "            self.weights_Path = \"bvlc_alexnet.npy\"  # put this file in same folder\n",
    "        else:\n",
    "            self.weights_Path = weights_path       # if not in same folder, fill the route \n",
    "        \n",
    "        self.buildCNN()\n",
    "        \n",
    "        \n",
    "    def buildCNN(self):\n",
    "        # first convolutional layer\n",
    "        # kernel = [11, 11, channel/groups, 96] and strides = [1, 4, 4, 1]\n",
    "        conv1 = conv2d(self.x, 11, 11, 4, 4, 96, \"conv1\", \"VALID\") \n",
    "        lrn1 = LRN(conv1, 2, 2e-05, 0.75, \"norm1\")  # less use\n",
    "        # kernel = [1, 3, 3, 1] strides =  [1, 2, 2, 1]\n",
    "        pool = pool2d(lrn1, 3, 3, 2, 2, \"pool1\", \"VALID\")\n",
    "        \n",
    "        # second convolutional layer(devide into 2 groups)\n",
    "        # kernel = [5, 5, 96/2, 256] and padding is same then output is 27x27x256\n",
    "        conv2 = conv2d(pool1, 5, 5, 1, 1, 256, \"conv2\", groups = 2)\n",
    "        lrn2 = LRN(conv2,  2, 2e-05, 0.75, \"norm2\")\n",
    "        # kernel = [1, 3, 3, 1] strides = [1, 2, 2, 1]\n",
    "        pool2 = pool2d(lrn2, 3, 3, 2, 2, \"pool2\", \"VALID\")\n",
    "        \n",
    "        # third convolutional layer\n",
    "        # kernel = [3, 3, 256, 384]  strides = [1, 1, 1, 1]\n",
    "        conv3 =  conv2d(pool2, 3, 3, 1, 1, 384, \"conv3\")\n",
    "        \n",
    "        # fourth convolutional layer\n",
    "        # kernel = [3, 3, 384/2, 384] strides = [1, 1, 1, 1], 2 groups, same padding\n",
    "        conv4 = conv2d(conv3, 3, 3, 1, 1, 384, \"conv4\", groups = 2)\n",
    "        \n",
    "        # fifth convolutional layer\n",
    "        conv5 = conv2d(conv4, 3, 3, 1, 1, 256, \"conv5\", groups = 2)\n",
    "        pool5 = pool2d(conv5, 3, 3, 2, 2, \"pool5\", \"VALID\")\n",
    "        \n",
    "        # reshape the convolution output to input FC layer\n",
    "        fcIn = tf.reshape(pool5, shape = [-1, 6*6*256])\n",
    "        # first fully connected layer\n",
    "        fc1 = fcLayer(fcIn, 256*6*6, 4096, True, \"fc6\")\n",
    "        drouput1 = dropout(fc1, keep_pro = self.keepPro)\n",
    "        # second fully connected layer\n",
    "        fc2 = fcLayer(dropout1, 4096, 4096, True, \"fc7\")\n",
    "        dropout2 = dropout(fc2, keep_pro = self.keepPro)\n",
    "        \n",
    "        #output of the model\n",
    "        self.out = fcLayer(dropout, 4096, self.classNum, False, \"fc8\")\n",
    "        \n",
    "    def load_initial_weights(self, session):\n",
    "        \n",
    "        # Load the pretrained weights and saves as a dict of list, \n",
    "        # like weights_dict[\"wc1\"] = XXXX\n",
    "        weights_dict = np.load(self.weights_Path, encoding = \"bytes\").item()\n",
    "        \n",
    "        # check if the layer should be trained \n",
    "        # if the variable should be trainable,then the data will be stored in collection\n",
    "        # and graph will get all data to train in next batch\n",
    "        for op_name in weights_dict:\n",
    "            if op_name not in self.skip:\n",
    "                with tf.variable_scope(op_name, reuse = True):  # get the weights from checkpoint file\n",
    "                    for data in weights_dict[op_name]:\n",
    "                        \n",
    "                        # bias\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable(\"biases\", trainable = false)\n",
    "                            session.run(var.assign(data))  # get the new pretrained data\n",
    "                        \n",
    "                        else:\n",
    "                            var = tf.get_variable(\"weights\", trainable = false)\n",
    "                            session.run(var.assign(data))\n",
    "                        \n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
